{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\dkostovic\\AppData\\Local\\Temp\\8\\ipykernel_11832\\1604288647.py\", line 19, in <module>\n",
      "    from helpers import helper_functions as hf\n",
      "  File \"H:\\all\\RL_Shrinkage_2024\\helpers\\helper_functions.py\", line 3, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cov2para_evs_df.csv', 'cov2para_fixed04_evs_df.csv', 'cov2para_plus02_evs_df.csv', 'qis_evs_df.csv', 'qis_evs_df_p100.csv', 'qis_evs_df_p225.csv', 'qis_evs_df_p30.csv', 'qis_evs_df_p50.csv', 'qis_evs_df_p500.csv', 'qis_evs_exp_05.csv', 'qis_evs_exp_2.csv', 'sample_evs_df.csv', 'sample_evs_df_p100.csv', 'sample_evs_df_p225.csv', 'sample_evs_df_p30.csv', 'sample_evs_df_p50.csv', 'sample_evs_df_p500.csv']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %%\n",
    "import psutil\n",
    "psutil.cpu_count()\n",
    "p = psutil.Process()\n",
    "p.cpu_affinity([0,1,2,3,4,5,6,7,8])\n",
    "\n",
    "# %%\n",
    "os.chdir(r'H:\\all\\RL_Shrinkage_2024')\n",
    "from helpers import helper_functions as hf\n",
    "\n",
    "# %%\n",
    "import glob\n",
    "\n",
    "path = r'H:\\all\\RL_Shrinkage_2024\\ONE_YR_long_only\\NonLinear_Shrinkage\\transformed_qis_eigenvalues'\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "filenames = glob.glob('*.{}'.format(extension))\n",
    "print(filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_new_eigenvalues(qis_eigenvalues, sample_eigenvalues, intensity_of_intensity):\n",
    "        intensity = qis_eigenvalues.copy()\n",
    "        if qis_eigenvalues.shape[1] == 500:\n",
    "            intensity.iloc[: , -251:] = qis_eigenvalues.iloc[: , -251:] / sample_eigenvalues.iloc[: , -251:]\n",
    "        else:\n",
    "            intensity = qis_eigenvalues / sample_eigenvalues\n",
    "        intensity_delta = intensity - 1\n",
    "        intensity_delta_new = intensity_delta * intensity_of_intensity\n",
    "        intensity_new = intensity_delta_new + 1\n",
    "        qis_evs_new = intensity_new * sample_eigenvalues\n",
    "        if qis_eigenvalues.shape[1] == 500:\n",
    "            qis_evs_new.iloc[: , 0:250] = intensity_of_intensity * qis_eigenvalues.iloc[: , 0:250]\n",
    "\n",
    "        # get Rotation Points\n",
    "        intens_df = qis_eigenvalues / sample_eigenvalues\n",
    "        if qis_eigenvalues.shape[1] == 500:  # only 1 year case ofcourse\n",
    "            intens_df = intens_df.fillna(0)\n",
    "        right_rotation_idx = np.argmin( np.abs(intens_df-1) , axis=1)\n",
    "        left_rotation_idx = right_rotation_idx - 1\n",
    "        rotation_point_sample_evs = 0.5 * np.diag(sample_eigenvalues.iloc[:, left_rotation_idx]) + 0.5 * np.diag(sample_eigenvalues.iloc[:, right_rotation_idx])\n",
    "        rotation_points = 0.5 * np.diag(intens_df.iloc[:, left_rotation_idx]) + 0.5 * np.diag(intens_df.iloc[:, right_rotation_idx])\n",
    "        rotation_points = rotation_points * rotation_point_sample_evs\n",
    "\n",
    "        # Force all eigenvalues to be in decreasing order (from the largest one)\n",
    "        idx_is_increasing = qis_evs_new.diff(axis=1).fillna(0) < 0\n",
    "        qis_evs_new[idx_is_increasing] = np.nan\n",
    "        qis_evs_new = qis_evs_new.bfill(axis=1)\n",
    "\n",
    "        # check if values left (right) of rotation point are smaller (larger) than rotation point\n",
    "        # check: is idx < left and value_at_idx > value_at_left --> then np.nan and ffill it for left (and bfill for right)\n",
    "        for i in range(qis_evs_new.shape[0]):\n",
    "            tmp = qis_evs_new.iloc[i, :].copy()\n",
    "            left_bool = tmp[0:left_rotation_idx[i]] > rotation_points[i]\n",
    "            right_bool = tmp[left_rotation_idx[i]:] < rotation_points[i]\n",
    "            # change those evs on right that are smaller than rotation point\n",
    "            tmp_right = tmp[left_rotation_idx[i]:]\n",
    "            tmp_right[right_bool] = np.nan\n",
    "            tmp_right.bfill(inplace=True) \n",
    "            tmp_right.ffill(inplace=True)  # in case the largest ev is NAN, we need to additionally ffill\n",
    "            # change those evs on left that are LARGER than rotation point\n",
    "            tmp_left = tmp[0:left_rotation_idx[i]]\n",
    "            tmp_left[left_bool] = np.nan\n",
    "            tmp_left.ffill(inplace=True) \n",
    "            tmp_left.bfill(inplace=True) # in case the smallest ev is NAN, we need to additionally bfill\n",
    "            tmp_left.fillna(rotation_points[i], inplace=True)  # in case all eigenvalues left of rotation point are larger than rotation point\n",
    "            # change qis_evs_new row\n",
    "            qis_evs_new.iloc[i, :] = np.concatenate([tmp_left, tmp_right])\n",
    "\n",
    "        # a few correction checks\n",
    "        assert any(qis_evs_new.diff(axis=1).fillna(0) < 0), \"Eigenvalues are not monotonically decreasing!\"\n",
    "        assert any(qis_evs_new.isna()), \"There are NaN's in the QIS Eigenvalue Matrix!\"\n",
    "    \n",
    "        return qis_evs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rawres_long_only(eigenvalue_dict, modelnames: list, idxlist: list):\n",
    "    tmp_res = defaultdict(list)\n",
    "    tmp_rawres = defaultdict(list)\n",
    "    weights_dict = dict()\n",
    "    for idx in idxlist:\n",
    "        try:\n",
    "            past_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx - 21 * 12 * 1: idx + add_idx, :]\n",
    "            past_ret_mat = past_ret_mat.sub(past_ret_mat.mean())\n",
    "            past_ret_mat = past_ret_mat.fillna(0)\n",
    "            fut_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx: idx + add_idx + 21, :]\n",
    "        except:\n",
    "            print(\"Some Error..\")\n",
    "            \n",
    "        N, p = past_ret_mat.shape\n",
    "        sample = pd.DataFrame(np.matmul(past_ret_mat.T.to_numpy(), past_ret_mat.to_numpy())) / (N - 1)\n",
    "        lambda1, u = np.linalg.eigh(sample)\n",
    "        lambda1 = lambda1.real.clip(min=0)\n",
    "        dfu = pd.DataFrame(u,columns=lambda1)\n",
    "        dfu.sort_index(axis=1,inplace = True)\n",
    "        temp1 = dfu.to_numpy()\n",
    "        temp3 = dfu.T.to_numpy().conjugate()\n",
    "\n",
    "        for cur_modelname in modelnames:\n",
    "            qis = eigenvalue_dict[cur_modelname].iloc[idx, :]\n",
    "            temp2 = np.diag(qis)\n",
    "            sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1, temp2), temp3))\n",
    "            try:\n",
    "                weights = hf.calc_global_min_variance_pf_long_only(sigmahat)\n",
    "                weights_dict[idx] = weights\n",
    "            except:\n",
    "                print(\"Some Other Error..\")\n",
    "            # store results\n",
    "            tmp_res[cur_modelname].append(np.std(fut_ret_mat @ weights, ddof=1) * np.sqrt(252) * 100)\n",
    "            if idx % 21 == 0:\n",
    "                tmp_rawres[cur_modelname] += list(fut_ret_mat @ weights)\n",
    "\n",
    "        if idx % 250 == 0:\n",
    "            print(f\"done {idx} out of {permnos.shape[0]}\")\n",
    "\n",
    "    return tmp_rawres, tmp_res, weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rawres_long_short(eigenvalue_dict, modelnames: list, idxlist: list):\n",
    "    tmp_res = defaultdict(list)\n",
    "    tmp_rawres = defaultdict(list)\n",
    "    weights_dict = dict()\n",
    "    for idx in idxlist:\n",
    "        try:\n",
    "            past_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx - 21 * 12 * 1: idx + add_idx, :]\n",
    "            past_ret_mat = past_ret_mat.sub(past_ret_mat.mean())\n",
    "            past_ret_mat = past_ret_mat.fillna(0)\n",
    "            fut_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx: idx + add_idx + 21, :]\n",
    "        except:\n",
    "            print(\"Some Error..\")\n",
    "            \n",
    "        N, p = past_ret_mat.shape\n",
    "        sample = pd.DataFrame(np.matmul(past_ret_mat.T.to_numpy(), past_ret_mat.to_numpy())) / (N - 1)\n",
    "        lambda1, u = np.linalg.eigh(sample)\n",
    "        lambda1 = lambda1.real.clip(min=0)\n",
    "        dfu = pd.DataFrame(u,columns=lambda1)\n",
    "        dfu.sort_index(axis=1,inplace = True)\n",
    "        temp1 = dfu.to_numpy()\n",
    "        temp3 = dfu.T.to_numpy().conjugate()\n",
    "\n",
    "        for cur_modelname in modelnames:\n",
    "            qis = eigenvalue_dict[cur_modelname].iloc[idx, :]\n",
    "            temp2 = np.diag(qis)\n",
    "            sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1, temp2), temp3))\n",
    "            try:\n",
    "                weights = hf.calc_global_min_variance_pf(sigmahat)\n",
    "                weights_dict[idx] = weights\n",
    "            except:\n",
    "                print(\"Some Other Error..\")\n",
    "            # store results\n",
    "            tmp_res[cur_modelname].append(np.std(fut_ret_mat @ weights, ddof=1) * np.sqrt(252) * 100)\n",
    "            if idx % 21 == 0:\n",
    "                tmp_rawres[cur_modelname] += list(fut_ret_mat @ weights)\n",
    "\n",
    "        if idx % 250 == 0:\n",
    "            print(f\"done {idx} out of {permnos.shape[0]}\")\n",
    "\n",
    "    return tmp_rawres, tmp_res, weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating new Eigenvalues..\n",
      "Finished Calculating new Eigenvalues\n"
     ]
    }
   ],
   "source": [
    "PF_SIZE = 500\n",
    "model_names = [\"qis\",  \"sample\"]\n",
    "evs_dfs_LONG_ONLY = {}\n",
    "qis_evs_path = r\"H:\\all\\RL_Shrinkage_2024\\ONE_YR_long_only\\NonLinear_Shrinkage\\transformed_qis_eigenvalues\"\n",
    "evs_dfs_LONG_ONLY[\"qis\"] = pd.read_csv(qis_evs_path + f'\\\\qis_evs_df_p{PF_SIZE}.csv', index_col=0)\n",
    "evs_dfs_LONG_ONLY[\"sample\"] = pd.read_csv(qis_evs_path + f'\\\\sample_evs_df_p{PF_SIZE}.csv', index_col=0)\n",
    "\n",
    "base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "permnos = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR_long_only\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{PF_SIZE}.pickle\")\n",
    "\n",
    "intensity_of_intensity_list = [0.5, 1.0, 1.5]\n",
    "print(\"Calculating new Eigenvalues..\")\n",
    "qis_evs_new_LONG_ONLY = {}\n",
    "intensities_new_LONG_ONLY = {}\n",
    "for intensity_of_intensity in intensity_of_intensity_list:\n",
    "    qis_evs_new_LONG_ONLY[intensity_of_intensity] = get_new_eigenvalues(evs_dfs_LONG_ONLY[\"qis\"], evs_dfs_LONG_ONLY[\"sample\"], intensity_of_intensity)\n",
    "print(\"Finished Calculating new Eigenvalues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating new Eigenvalues..\n",
      "Finished Calculating new Eigenvalues\n"
     ]
    }
   ],
   "source": [
    "PF_SIZE = 500\n",
    "model_names = [\"qis\",  \"sample\"]\n",
    "evs_dfs_LONG_SHORT = {}\n",
    "qis_evs_path = r\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\NonLinear_Shrinkage\\transformed_qis_eigenvalues\"\n",
    "evs_dfs_LONG_SHORT[\"qis\"] = pd.read_csv(qis_evs_path + f'\\\\qis_evs_df_p{PF_SIZE}.csv', index_col=0)\n",
    "evs_dfs_LONG_SHORT[\"sample\"] = pd.read_csv(qis_evs_path + f'\\\\sample_evs_df_p{PF_SIZE}.csv', index_col=0)\n",
    "\n",
    "base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "permnos = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{PF_SIZE}.pickle\")\n",
    "\n",
    "intensity_of_intensity_list = [0.5, 1.0, 1.5]\n",
    "print(\"Calculating new Eigenvalues..\")\n",
    "qis_evs_new_LONG_SHORT = {}\n",
    "intensities_new_LONG_SHORT = {}\n",
    "for intensity_of_intensity in intensity_of_intensity_list:\n",
    "    qis_evs_new_LONG_SHORT[intensity_of_intensity] = get_new_eigenvalues(evs_dfs_LONG_SHORT[\"qis\"], evs_dfs_LONG_SHORT[\"sample\"], intensity_of_intensity)\n",
    "print(\"Finished Calculating new Eigenvalues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating res and rawres matrices for PF SIZE: 500..\n",
      "done 5250 out of 10353\n"
     ]
    }
   ],
   "source": [
    "### long only\n",
    "if 1 == 1:\n",
    "    import pickle\n",
    "    base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "    # IMPORT SHRK DATASETS\n",
    "    pf_size = PF_SIZE  # DONT CHANGE HERE!!\n",
    "    permnos = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR_long_only\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{pf_size}.pickle\")\n",
    "    rets_full = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR_long_only\\preprocessing\\rets_permnos_1Y\\returns_full_1Y_p{pf_size}.pickle\")\n",
    "\n",
    "    fixed_shrk_name = 'cov1Para'\n",
    "    opt_shrk_name = 'cov1Para'\n",
    "    with open(rf\"{base_folder_path}\\ONE_YR_long_only\\preprocessing\\training_dfs\\PF{pf_size}\\fixed_shrkges_{fixed_shrk_name}_p{pf_size}.pickle\", 'rb') as f:\n",
    "        fixed_shrk_data = pickle.load(f)\n",
    "    with open(rf\"{base_folder_path}\\ONE_YR_long_only\\preprocessing\\training_dfs\\PF{pf_size}\\{opt_shrk_name}_factor-1.0_p{pf_size}.pickle\", 'rb') as f:\n",
    "        optimal_shrk_data = pickle.load(f)\n",
    "\n",
    "    # get all the validation indices\n",
    "    len_train = 5040\n",
    "    end_date = fixed_shrk_data.shape[0]\n",
    "    # temp here\n",
    "    val_indices_correct = (len_train, end_date)\n",
    "    val_indices_results = [val_indices_correct[0] + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "    val_idxes_shrkges = [0 + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "    reb_date_1 = permnos.index[0]\n",
    "    add_idx = np.where(rets_full.index == reb_date_1)[0][0]\n",
    "\n",
    "    # %%\n",
    "    print(f\"creating res and rawres matrices for PF SIZE: {PF_SIZE}..\")\n",
    "    idxlist = list(range(5040, 10353, 21))\n",
    "    tmp_rawres, tmp_res, WEIGHTS_LONG_ONLY =  get_rawres_long_only(qis_evs_new_LONG_ONLY, modelnames = intensity_of_intensity_list, idxlist=idxlist)\n",
    "    all_res_LONG_ONLY = pd.DataFrame(tmp_res.copy())\n",
    "    all_rawres_LONG_ONLY = pd.DataFrame(tmp_rawres.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating res and rawres matrices for PF SIZE: 500..\n",
      "done 5250 out of 10353\n"
     ]
    }
   ],
   "source": [
    "### long short\n",
    "if 1 == 1:\n",
    "    import pickle\n",
    "    base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "    # IMPORT SHRK DATASETS\n",
    "    pf_size = PF_SIZE  # DONT CHANGE HERE!!\n",
    "    permnos = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{pf_size}.pickle\")\n",
    "    rets_full = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\returns_full_1Y_p{pf_size}.pickle\")\n",
    "\n",
    "    fixed_shrk_name = 'cov1Para'\n",
    "    opt_shrk_name = 'cov1Para'\n",
    "    with open(rf\"{base_folder_path}\\ONE_YR\\preprocessing\\training_dfs\\PF{pf_size}\\fixed_shrkges_{fixed_shrk_name}_p{pf_size}.pickle\", 'rb') as f:\n",
    "        fixed_shrk_data = pickle.load(f)\n",
    "    with open(rf\"{base_folder_path}\\ONE_YR\\preprocessing\\training_dfs\\PF{pf_size}\\{opt_shrk_name}_factor-1.0_p{pf_size}.pickle\", 'rb') as f:\n",
    "        optimal_shrk_data = pickle.load(f)\n",
    "\n",
    "    # get all the validation indices\n",
    "    len_train = 5040\n",
    "    end_date = fixed_shrk_data.shape[0]\n",
    "    # temp here\n",
    "    val_indices_correct = (len_train, end_date)\n",
    "    val_indices_results = [val_indices_correct[0] + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "    val_idxes_shrkges = [0 + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "    reb_date_1 = permnos.index[0]\n",
    "    add_idx = np.where(rets_full.index == reb_date_1)[0][0]\n",
    "\n",
    "    # %%\n",
    "    print(f\"creating res and rawres matrices for PF SIZE: {PF_SIZE}..\")\n",
    "    idxlist = list(range(5040, 10353, 21))\n",
    "    tmp_rawres, tmp_res, WEIGHTS_LONG_SHORT =  get_rawres_long_short(qis_evs_new_LONG_SHORT, modelnames = intensity_of_intensity_list, idxlist=idxlist)\n",
    "    all_res_LONG_SHORT = pd.DataFrame(tmp_res.copy())\n",
    "    all_rawres_LONG_SHORT = pd.DataFrame(tmp_rawres.copy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the weights\n",
    "- also find out why the long short version takes so much longer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    10.615772\n",
       "1.0    10.376136\n",
       "1.5    10.411982\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rawres_LONG_SHORT.std() * np.sqrt(252) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    12.148598\n",
       "1.0    12.242208\n",
       "1.5    12.359996\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_rawres_LONG_ONLY.std() * np.sqrt(252) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS_LONG_SHORT[5040].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS_LONG_SHORT[5040][ (WEIGHTS_LONG_SHORT[5040] < 1e-5) & (WEIGHTS_LONG_SHORT[5040] > -1e-5) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0000000000000007)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS_LONG_ONLY[5040].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### transform weights long only\n",
    "\n",
    "w_long_copies = {}\n",
    "for k, v in WEIGHTS_LONG_ONLY.items():\n",
    "    w_long_copies[k] = v.copy()\n",
    "    w_long_copies[k][(w_long_copies[k] < 1e-4) & (w_long_copies[k] > -1e-4) ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0000000000000007)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_long_copies[5040].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GET_RAWRES_LONG_ONLY_TEMPORARY(eigenvalue_dict, modelnames: list, idxlist: list, input_weights: dict):\n",
    "    tmp_res = defaultdict(list)\n",
    "    tmp_rawres = defaultdict(list)\n",
    "    weights_dict = dict()\n",
    "    for idx in idxlist:\n",
    "\n",
    "        try:\n",
    "            past_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx - 21 * 12 * 1: idx + add_idx, :]\n",
    "            past_ret_mat = past_ret_mat.sub(past_ret_mat.mean())\n",
    "            past_ret_mat = past_ret_mat.fillna(0)\n",
    "            fut_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx: idx + add_idx + 21, :]\n",
    "        except:\n",
    "            print(\"Some Error..\")\n",
    "        for cur_modelname in modelnames:\n",
    "            try:\n",
    "                weights = input_weights[idx]\n",
    "                weights_dict[idx] = weights\n",
    "            except:\n",
    "                print(\"Some Other Error..\")\n",
    "            # store results\n",
    "            tmp_res[cur_modelname].append(np.std(fut_ret_mat @ weights, ddof=1) * np.sqrt(252) * 100)\n",
    "            if idx % 21 == 0:\n",
    "                tmp_rawres[cur_modelname] += list(fut_ret_mat @ weights)\n",
    "\n",
    "        if idx % 250 == 0:\n",
    "            print(f\"done {idx} out of {permnos.shape[0]}\")\n",
    "\n",
    "    return tmp_rawres, tmp_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 5250 out of 10353\n"
     ]
    }
   ],
   "source": [
    "tmp_rawres, tmp_res = GET_RAWRES_LONG_ONLY_TEMPORARY(qis_evs_new_LONG_ONLY, modelnames=[1.5], idxlist=idxlist, input_weights=WEIGHTS_LONG_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(12.358832656698091)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tmp_rawres[1.5]).std()*np.sqrt(252) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARE QIS EVS LONG ONLY WITH QIS EVS USUAL CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "def calc_global_min_variance_pf_long_only_TESTING(covmat_estimator):\n",
    "    \"\"\"\n",
    "    Calculates the global minimum portfolio WITH SHORT SELLING CONSTRAINTS\n",
    "    :param covmat_estimator: covariance matrix estimator of shape p x p\n",
    "    :return: portfolio weights\n",
    "    \"\"\"\n",
    "    # since I know that my covmat estimator is psd I can add the line below\n",
    "    # usual optimization\n",
    "\n",
    "    if isinstance(covmat_estimator, pd.DataFrame):\n",
    "        covmat_estimator = covmat_estimator.values  # or .to_numpy()\n",
    "\n",
    "    w = cp.Variable(covmat_estimator.shape[0])  \n",
    "    risk = cp.quad_form(w, covmat_estimator, assume_PSD=True)\n",
    "    objective = cp.Minimize(risk)\n",
    "    constraints = [cp.sum(w) == 1, w >= 0]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "    # result is stored in the cp.Variable (w) after solving\n",
    "    return w.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rawres_long_only_TEMPORARY_V2(eigenvalue_dict, modelnames: list, idxlist: list):\n",
    "    tmp_res = defaultdict(list)\n",
    "    tmp_rawres = defaultdict(list)\n",
    "    weights_dict = dict()\n",
    "    for idx in idxlist:\n",
    "        try:\n",
    "            past_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx - 21 * 12 * 1: idx + add_idx, :]\n",
    "            past_ret_mat = past_ret_mat.sub(past_ret_mat.mean())\n",
    "            past_ret_mat = past_ret_mat.fillna(0)\n",
    "            fut_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx: idx + add_idx + 21, :]\n",
    "        except:\n",
    "            print(\"Some Error..\")\n",
    "            \n",
    "        N, p = past_ret_mat.shape\n",
    "        sample = pd.DataFrame(np.matmul(past_ret_mat.T.to_numpy(), past_ret_mat.to_numpy())) / (N - 1)\n",
    "        lambda1, u = np.linalg.eigh(sample)\n",
    "        lambda1 = lambda1.real.clip(min=0)\n",
    "        dfu = pd.DataFrame(u,columns=lambda1)\n",
    "        dfu.sort_index(axis=1,inplace = True)\n",
    "        temp1 = dfu.to_numpy()\n",
    "        temp3 = dfu.T.to_numpy().conjugate()\n",
    "\n",
    "        for cur_modelname in modelnames:\n",
    "            qis = eigenvalue_dict[cur_modelname].iloc[idx, :]\n",
    "            temp2 = np.diag(qis)\n",
    "            sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1, temp2), temp3))\n",
    "            try:\n",
    "                weights = calc_global_min_variance_pf_long_only_TESTING(sigmahat)\n",
    "                weights_dict[idx] = weights\n",
    "            except:\n",
    "                print(\"Some Other Error..\")\n",
    "            # store results\n",
    "            tmp_res[cur_modelname].append(np.std(fut_ret_mat @ weights, ddof=1) * np.sqrt(252) * 100)\n",
    "            if idx % 21 == 0:\n",
    "                tmp_rawres[cur_modelname] += list(fut_ret_mat @ weights)\n",
    "\n",
    "        if idx % 250 == 0:\n",
    "            print(f\"done {idx} out of {permnos.shape[0]}\")\n",
    "\n",
    "    return tmp_rawres, tmp_res, weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 5250 out of 10353\n"
     ]
    }
   ],
   "source": [
    "tmp_rawres, tmp_res, weights_dict = get_rawres_long_only_TEMPORARY_V2(eigenvalue_dict=qis_evs_new_LONG_ONLY, modelnames=[0.5, 1.0, 1.5], idxlist=idxlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5    12.148628\n",
       "1.0    12.242847\n",
       "1.5    12.359996\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tmp_rawres).std()* np.sqrt(252)*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FTSE_data_analysis_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
