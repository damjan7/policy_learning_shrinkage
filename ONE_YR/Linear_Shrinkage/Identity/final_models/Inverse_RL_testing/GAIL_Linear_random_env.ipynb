{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "import psutil\n",
    "psutil.cpu_count()\n",
    "p = psutil.Process()\n",
    "p.cpu_affinity([0,1,2,3,4,5,6,7,8 ])\n",
    "\n",
    "os.chdir(r'H:\\all\\RL_Shrinkage_2024')\n",
    "from helpers import helper_functions as hf\n",
    "from ONE_YR.NonLinear_Shrinkage import regression_evaluation_funcs as re_hf\n",
    "from helpers import eval_function_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PF_SIZE = 500\n",
    "\n",
    "\n",
    "base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "# IMPORT SHRK DATASETS\n",
    "pf_size = PF_SIZE  # DONT CHANGE HERE!!\n",
    "permnos = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{pf_size}.pickle\")\n",
    "rets_full = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\returns_full_1Y_p{pf_size}.pickle\")\n",
    "\n",
    "\n",
    "fixed_shrk_name = 'cov1Para'\n",
    "opt_shrk_name = 'cov1Para'\n",
    "with open(rf\"{base_folder_path}\\ONE_YR\\preprocessing\\training_dfs\\PF{pf_size}\\fixed_shrkges_{fixed_shrk_name}_p{pf_size}.pickle\", 'rb') as f:\n",
    "    fixed_shrk_data = pickle.load(f)\n",
    "with open(rf\"{base_folder_path}\\ONE_YR\\preprocessing\\training_dfs\\PF{pf_size}\\{opt_shrk_name}_factor-1.0_p{pf_size}.pickle\", 'rb') as f:\n",
    "    optimal_shrk_data = pickle.load(f)\n",
    "\n",
    "with open(rf\"{base_folder_path}\\ONE_YR\\preprocessing\\training_dfs\\PF{pf_size}\\fixed_shrkges_rawres_{fixed_shrk_name}_p{pf_size}.pickle\", 'rb') as f:\n",
    "    rawres_fixed_shrk_data = pickle.load(f)\n",
    "\n",
    "# IMPORT FACTORS DATA AND PREPARE FOR FURTHER USE\n",
    "factor_path = fr\"{base_folder_path}\\helpers\"\n",
    "factors = pd.read_csv(factor_path + \"/all_factors.csv\")\n",
    "factors = factors.pivot(index=\"date\", columns=\"name\", values=\"ret\")\n",
    "\n",
    "# as our shrk data starts from 1980-01-15 our factors data should too\n",
    "start_date = str(optimal_shrk_data['date'].iloc[0])\n",
    "start_date = start_date[0:4] + '-' + start_date[4:6] + \"-\" + start_date[6:]\n",
    "start_idx = np.where(factors.index == start_date)[0][0]\n",
    "factors = factors.iloc[start_idx:start_idx+fixed_shrk_data.shape[0], :]\n",
    "\n",
    "cov1para_shrk = optimal_shrk_data['shrk_factor'].values[5040:]\n",
    "\n",
    "len_train = 5040\n",
    "end_date = fixed_shrk_data.shape[0]\n",
    "val_indices_correct = (len_train, end_date)\n",
    "val_indices_results = [val_indices_correct[0] + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "val_idxes_shrkges = [0 + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "\n",
    "\n",
    "all_res = fixed_shrk_data.iloc[:, 2:].copy() * 100\n",
    "all_factors = all_res.columns.astype(float).values\n",
    "Y = all_res.idxmin(axis=1).values.astype(float)\n",
    "opt_values = all_res.idxmin(axis=1).values.astype(float)[:-21]\n",
    "opt_values = np.insert(arr=opt_values, obj=0, values=np.repeat(0.15, 21))\n",
    "Y = np.array(re_hf.map_factors_to_preds(Y.reshape(-1), all_factors))\n",
    "opt_values = np.array(re_hf.map_factors_to_preds(opt_values, all_factors))\n",
    "opt_v3 = np.diag(all_res.loc[:, all_res.idxmin(axis=1).values])[:-21]\n",
    "opt_v3 = np.insert(arr=opt_v3, obj=0, values=np.repeat(7.0, 21))\n",
    "\n",
    "rolling_opt = pd.Series(opt_values).rolling(window=252, min_periods=1).mean().values\n",
    "rolling_opt2 = pd.Series(opt_v3).rolling(window=252, min_periods=1).mean().values\n",
    "\n",
    "\n",
    "ROLL_WINDOW = 1\n",
    "ROLL_WINDOW_INPUTS = 252\n",
    "rolling_opt = pd.Series(opt_values).rolling(window=ROLL_WINDOW_INPUTS, min_periods=1).mean().values\n",
    "rolling_opt2 = pd.Series(opt_v3).rolling(window=ROLL_WINDOW_INPUTS, min_periods=1).mean().values\n",
    "\n",
    "Y = pd.Series(Y).rolling(window=1, min_periods=1).mean().astype(np.int64).values\n",
    "opt_values = pd.Series(opt_values).rolling(window=1, min_periods=1).mean().astype(np.int64).values\n",
    "\n",
    "params = {\n",
    "'pf_size' : pf_size,\n",
    "'opt_values_factors' : opt_values,\n",
    "'include_ts_momentum_var_allstocks': False,\n",
    "'include_ts_momentum_allstocks': True,\n",
    "'include_sample_covmat_trace': True,\n",
    "'include_mean_of_correls': True,\n",
    "'include_iqr': False,\n",
    "'include_factors': False,\n",
    "'include_ewma_year': False,\n",
    "'include_ewma_month': True,\n",
    "'include_ew_year_vola': False,\n",
    "'include_ew_month_vola': True,\n",
    "'include_allstocks_year_avgvola': True,\n",
    "'include_allstocks_month_avgvola': False,\n",
    "    'additional_inputs' : [opt_v3, rolling_opt, rolling_opt2, optimal_shrk_data['shrk_factor'].values.astype(np.float64)*100]  \n",
    "}\n",
    "\n",
    "X = re_hf.load_additional_train_data(**params)\n",
    "X=X.astype(np.float64)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define all needed data structures to use the imitation package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from typing import Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 3 1]\n",
      "2\n",
      "[0 2 3 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.choice(range(5), 5)\n",
    "print(a)\n",
    "new = a[0]\n",
    "a=a[1:]\n",
    "print(new)\n",
    "print(a)\n",
    "\n",
    "[1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2:].any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not [1]:\n",
    "    print(\"empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment TimeSeriesEnv already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n",
      "c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:175: UserWarning: \u001b[33mWARN: The default seed argument in `Env.reset` should be `None`, otherwise the environment will by default always be deterministic. Actual default: seed=412\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'obs': array([ 1.50000000e+01,  1.63444714e-01,  5.90436631e+01,  3.31176955e-01,\n",
       "         9.96200000e+00, -1.95747619e-02,  2.22815320e-01,  7.00000000e+00,\n",
       "         1.50000000e+01,  7.00000000e+00,  7.42271121e+00]),\n",
       " 'acts': 60,\n",
       " 'infos': {},\n",
       " 'next_obs': array([ 1.50000000e+01,  1.63444714e-01,  5.90436631e+01,  3.31176955e-01,\n",
       "         9.96200000e+00, -1.95747619e-02,  2.22815320e-01,  7.00000000e+00,\n",
       "         1.50000000e+01,  7.00000000e+00,  7.42271121e+00]),\n",
       " 'dones': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TimeSeriesEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Exog. time-series; actions only affect rewards, not future obs.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.array, expert_actions: np.array, reward_fn: callable, train_size: np.array):\n",
    "        self.state = None\n",
    "        # save whole train data in env.\n",
    "        self.data = data\n",
    "        self.T, self.obs_dim = data.shape\n",
    "        self.observation_space = Box(-np.inf, np.inf, shape=(self.obs_dim, ) )\n",
    "        self.action_space = Discrete(n=101) # shrinkage between 0.0 and 1.0, 101 steps\n",
    "        self.reward_fn = reward_fn\n",
    "        self.expert_actions = expert_actions\n",
    "        self.train_size = train_size\n",
    "    \n",
    "    def reset(self, seed=412, **kwargs):\n",
    "        \"\"\"\n",
    "        reset env to an initial state, in our case start of t.s. or just random?\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.training_order = np.random.choice(range(self.train_size), self.train_size)\n",
    "        self.idx = self.training_order[0]\n",
    "        #self.state = self.observation_space.sample()\n",
    "        self.state = self.data[self.idx]\n",
    "        return self.state, {}  # 'self.state' can be OOS standard deviation?\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        from Gymnasium docs;\n",
    "        Updates an environment with actions returning the next agent observation, \n",
    "        the reward for taking that actions, if the environment has terminated or \n",
    "        truncated due to the latest action and information from the environment \n",
    "        about the step, i.e. metrics, debug info.\n",
    "        \"\"\"\n",
    "        cur_state = self.data[self.idx]\n",
    "        reward = self.reward_fn([self.expert_actions[self.idx]], [action])\n",
    "        # next state\n",
    "        self.training_order = self.training_order[1:]\n",
    "        if not self.training_order.any():\n",
    "            done = True\n",
    "            next_state = np.zeros_like(cur_state)\n",
    "        else:\n",
    "            done = False\n",
    "            self.idx = self.training_order[1:]\n",
    "            next_state = self.data[self.idx].copy()\n",
    "\n",
    "        # terminate after one step hence --> can just return None for next_state?\n",
    "        return next_state, reward, done, False, {}\n",
    "    \n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# register, vectorize, and wrap env to work with imitation apackage\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "gym.register(\n",
    "    id=\"TimeSeriesEnv\",\n",
    "    entry_point=lambda: TimeSeriesEnv(data=X, reward_fn=mean_squared_error, expert_actions=Y, train_size=len_train),\n",
    "    max_episode_steps=len_train,\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "venv = make_vec_env(\n",
    "    env_name=\"TimeSeriesEnv\",\n",
    "    rng=rng,\n",
    "    n_envs=4,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n",
    ")\n",
    "\n",
    "from imitation.data.types import Transitions\n",
    "flattened_transitions=Transitions(\n",
    "    obs=X[:len_train], \n",
    "    acts=Y[:len_train], \n",
    "    infos=np.array([{} for _ in range(len_train)]),\n",
    "    next_obs=X[:len_train],\n",
    "    dones=np.array([True for _ in range(len_train)])\n",
    "    )\n",
    "\n",
    "flattened_transitions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:   0%|          | 0/1220 [00:00<?, ?it/s]c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:135: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "round:   0%|          | 0/1220 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (5038,11) into shape (11,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 30\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m# 4) GAIL trainer\u001b[39;00m\n\u001b[0;32m     23\u001b[0m gail \u001b[39m=\u001b[39m GAIL(\n\u001b[0;32m     24\u001b[0m     demonstrations\u001b[39m=\u001b[39mflattened_transitions,\n\u001b[0;32m     25\u001b[0m     demo_batch_size\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     venv\u001b[39m=\u001b[39mvenv,\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m gail\u001b[39m.\u001b[39;49mtrain(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m10_000_000\u001b[39;49m)\n\u001b[0;32m     31\u001b[0m gail_policy \u001b[39m=\u001b[39m gail\u001b[39m.\u001b[39mgen_algo\u001b[39m.\u001b[39mpolicy\n\u001b[0;32m     34\u001b[0m \u001b[39m# register, vectorize, and wrap env to work with imitation apackage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\imitation\\algorithms\\adversarial\\common.py:454\u001b[0m, in \u001b[0;36mAdversarialTrainer.train\u001b[1;34m(self, total_timesteps, callback)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39massert\u001b[39;00m n_rounds \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, (\n\u001b[0;32m    449\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNo updates (need at least \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    450\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_train_timesteps\u001b[39m}\u001b[39;00m\u001b[39m timesteps, have only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtotal_timesteps=\u001b[39m\u001b[39m{\u001b[39;00mtotal_timesteps\u001b[39m}\u001b[39;00m\u001b[39m)!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m )\n\u001b[0;32m    453\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n_rounds), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mround\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 454\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_gen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen_train_timesteps)\n\u001b[0;32m    455\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_disc_updates_per_round):\n\u001b[0;32m    456\u001b[0m         \u001b[39mwith\u001b[39;00m networks\u001b[39m.\u001b[39mtraining(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_train):\n\u001b[0;32m    457\u001b[0m             \u001b[39m# switch to training mode (affects dropout, normalization)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\imitation\\algorithms\\adversarial\\common.py:414\u001b[0m, in \u001b[0;36mAdversarialTrainer.train_gen\u001b[1;34m(self, total_timesteps, learn_kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m     learn_kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m    413\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39maccumulate_means(\u001b[39m\"\u001b[39m\u001b[39mgen\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 414\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_algo\u001b[39m.\u001b[39mlearn(\n\u001b[0;32m    415\u001b[0m         total_timesteps\u001b[39m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    416\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    417\u001b[0m         callback\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_callback,\n\u001b[0;32m    418\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlearn_kwargs,\n\u001b[0;32m    419\u001b[0m     )\n\u001b[0;32m    420\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_global_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    422\u001b[0m gen_trajs, ep_lens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvenv_buffering\u001b[39m.\u001b[39mpop_trajectories()\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    316\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    317\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    318\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    319\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    320\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    321\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 277\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m continue_training:\n\u001b[0;32m    280\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:194\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m         \u001b[39m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    191\u001b[0m         \u001b[39m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 194\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    198\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\imitation\\rewards\\reward_wrapper.py:93\u001b[0m, in \u001b[0;36mRewardVecEnvWrapper.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m     obs, old_rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[0;32m     95\u001b[0m     \u001b[39m# The vecenvs automatically reset the underlying environments once they\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# encounter a `done`, in which case the last observation corresponding to\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# the `done` is dropped. We're going to pull it back out of the info dict!\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     obs_fixed \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\imitation\\data\\wrappers.py:73\u001b[0m, in \u001b[0;36mBufferingWrapper.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_saved_acts \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     72\u001b[0m acts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_saved_acts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_saved_acts, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m obs, rews, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[0;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_transitions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:71\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n\u001b[0;32m     70\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mreset()\n\u001b[1;32m---> 71\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_obs(env_idx, obs)\n\u001b[0;32m     72\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf(), np\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews), np\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones), deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos))\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:108\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[1;34m(self, env_idx, obs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys:\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs\n\u001b[0;32m    109\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs[key]\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (5038,11) into shape (11,)"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "\n",
    "# 1) Expert rollouts already in `expert_rollouts`\n",
    "# 2) Generator\n",
    "gen_algo = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=venv,\n",
    "    batch_size=64,\n",
    "    n_epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "    gamma=0.99,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "# 3) Discriminator\n",
    "disc_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "# 4) GAIL trainer\n",
    "gail = GAIL(\n",
    "    demonstrations=flattened_transitions,\n",
    "    demo_batch_size=1024,\n",
    "    gen_algo=gen_algo,\n",
    "    reward_net=disc_net,\n",
    "    venv=venv,\n",
    ")\n",
    "gail.train(total_timesteps=200_000)\n",
    "gail_policy = gail.gen_algo.policy\n",
    "\n",
    "\n",
    "# register, vectorize, and wrap env to work with imitation apackage\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "\n",
    "gym.register(\n",
    "    id=\"TimeSeriesEnv\",\n",
    "    entry_point=lambda: TimeSeriesEnv(data=X, reward_fn=mean_squared_error, expert_actions=Y),\n",
    "    max_episode_steps=len_train,\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "venv = make_vec_env(\n",
    "    env_name=\"TimeSeriesEnv\",\n",
    "    rng=rng,\n",
    "    n_envs=4,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FTSE_data_analysis_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
