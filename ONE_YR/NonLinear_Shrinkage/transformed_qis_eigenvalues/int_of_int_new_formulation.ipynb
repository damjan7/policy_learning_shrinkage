{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c9de18-e633-49f9-92b0-a185629e12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b93c80-118c-443f-80aa-0e173b3b220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "psutil.cpu_count()\n",
    "p = psutil.Process()\n",
    "p.cpu_affinity([0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019defae-95ec-4ea1-be14-6bb76ac28708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\dkostovic\\AppData\\Local\\Temp\\3\\ipykernel_30180\\2409205078.py\", line 2, in <module>\n",
      "    from helpers import helper_functions as hf\n",
      "  File \"H:\\all\\RL_Shrinkage_2024\\helpers\\helper_functions.py\", line 3, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "os.chdir(r'H:\\all\\RL_Shrinkage_2024')\n",
    "from helpers import helper_functions as hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac422b8-3127-4519-8772-db8985eee9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cov2para_evs_df.csv', 'cov2para_fixed04_evs_df.csv', 'cov2para_plus02_evs_df.csv', 'qis_evs_df.csv', 'qis_evs_df_p100.csv', 'qis_evs_df_p225.csv', 'qis_evs_df_p30.csv', 'qis_evs_df_p50.csv', 'qis_evs_df_p500.csv', 'qis_evs_exp_05.csv', 'qis_evs_exp_2.csv', 'sample_evs_df.csv', 'sample_evs_df_p100.csv', 'sample_evs_df_p225.csv', 'sample_evs_df_p30.csv', 'sample_evs_df_p50.csv', 'sample_evs_df_p500.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "path = r'H:\\all\\RL_Shrinkage_2024\\ONE_YR\\NonLinear_Shrinkage\\transformed_qis_eigenvalues'\n",
    "extension = 'csv'\n",
    "os.chdir(path)\n",
    "filenames = glob.glob('*.{}'.format(extension))\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a59aff-63ed-4d6e-9af5-f0474d28f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ev_intensities(evs, sample_evs):\n",
    "    intensities = evs.copy()\n",
    "    if sample_evs.shape[1] == 500:\n",
    "        intensities.iloc[:, -251:] = evs.iloc[:, -251:].values / sample_evs.iloc[:, -251:].values\n",
    "    else:\n",
    "        intensities = evs.values / sample_evs.values\n",
    "    return intensities\n",
    "\n",
    "def plot_last_x_intensities_across_time(intensities_dfs, modelnames, num_evs, return_df=False):\n",
    "    if len(modelnames) > 1:\n",
    "        dfs_list = [intensities_dfs[M].iloc[:, -num_evs:].mean(axis=1) for M in modelnames]\n",
    "        df = pd.concat(dfs_list, axis=1)\n",
    "        df.columns = modelnames\n",
    "    else:\n",
    "        df = intensities_dfs[modelnames[0]].iloc[:, -num_evs:].mean(axis=1)\n",
    "        df.name = modelnames[0]\n",
    "    fig = px.line(\n",
    "        df,\n",
    "        title = f\"Plot of Average EV(Model) / EV(Sample) Across Time\",\n",
    "         labels={'value':f'Mean of EV(Model) / EV(Sample)'},\n",
    "        height=700\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def plot_last_x_intensities_across_assets(intensities_dfs, modelnames, num_evs, return_df=False):\n",
    "    if len(modelnames) > 1:\n",
    "        dfs_list = [intensities_dfs[M].iloc[:, -num_evs:].mean() for M in modelnames]\n",
    "        df = pd.concat(dfs_list, axis=1)\n",
    "        df.columns = modelnames\n",
    "    else:\n",
    "        df = intensities_dfs[modelnames[0]].iloc[:, -num_evs:].mean()\n",
    "        df.name = modelnames[0]\n",
    "        \n",
    "    fig = px.line(\n",
    "        df,\n",
    "        title = f\"Plot of Average EV(Model) / EV(Sample) Across Assets\",\n",
    "         labels={'value':f'Mean of EV(Model) / EV(Sample)', 'index': \"Eigenvalue\"},\n",
    "        height=700\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_last_x_eigenvalues_across_time(eigenvalues_dfs, modelnames, ev_range, return_df=False):\n",
    "    if len(modelnames) > 1:\n",
    "        dfs_list = [eigenvalues_dfs[M].iloc[:, ev_range[0]:ev_range[1]].mean(axis=1) for M in modelnames]\n",
    "        df = pd.concat(dfs_list, axis=1)\n",
    "        df.columns = modelnames\n",
    "    else:\n",
    "        df = eigenvalues_dfs[modelnames[0]].iloc[:, ev_range[0]:ev_range[1]].mean(axis=1)\n",
    "        df.name = modelnames[0]\n",
    "    fig = px.line(\n",
    "        df,\n",
    "        title = f\"Plot of Average EV(Model) Across Time\",\n",
    "         labels={'value':f'Mean of EV(Model) '},\n",
    "        height=700\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def plot_last_x_eigenvalues_across_assets(eigenvalues_dfs, modelnames, ev_range, return_df=False):\n",
    "    if len(modelnames) > 1:\n",
    "        dfs_list = [eigenvalues_dfs[M].iloc[:, ev_range[0]:ev_range[1]].mean() for M in modelnames]\n",
    "        df = pd.concat(dfs_list, axis=1)\n",
    "        df.columns = modelnames\n",
    "    else:\n",
    "        df = eigenvalues_dfs[modelnames[0]].iloc[:, ev_range[0]:ev_range[1]].mean()\n",
    "        df.name = modelnames[0]\n",
    "        \n",
    "    fig = px.line(\n",
    "        df,\n",
    "        title = f\"Plot of Average EV(Model) Across Assets\",\n",
    "         labels={'value':f'Mean of EV(Model)', 'index': \"Eigenvalue\"},\n",
    "        height=700\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def get_rawres(eigenvalue_dict, modelnames: list):\n",
    "    tmp_res = defaultdict(list)\n",
    "    tmp_rawres = defaultdict(list)\n",
    "    for idx in range(0, permnos.shape[0]):\n",
    "        try:\n",
    "            past_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx - 21 * 12 * 1: idx + add_idx, :]\n",
    "            past_ret_mat = past_ret_mat.sub(past_ret_mat.mean())\n",
    "            past_ret_mat = past_ret_mat.fillna(0)\n",
    "            fut_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx: idx + add_idx + 21, :]\n",
    "        except:\n",
    "            print(\"Some Error..\")\n",
    "            \n",
    "        N, p = past_ret_mat.shape\n",
    "        sample = pd.DataFrame(np.matmul(past_ret_mat.T.to_numpy(), past_ret_mat.to_numpy())) / (N - 1)\n",
    "        lambda1, u = np.linalg.eigh(sample)\n",
    "        lambda1 = lambda1.real.clip(min=0)\n",
    "        dfu = pd.DataFrame(u,columns=lambda1)\n",
    "        dfu.sort_index(axis=1,inplace = True)\n",
    "        temp1 = dfu.to_numpy()\n",
    "        temp3 = dfu.T.to_numpy().conjugate()\n",
    "\n",
    "        for cur_modelname in modelnames:\n",
    "            qis = eigenvalue_dict[cur_modelname].iloc[idx, :]\n",
    "            temp2 = np.diag(qis)\n",
    "            sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1, temp2), temp3))\n",
    "            try:\n",
    "                weights = hf.calc_global_min_variance_pf(sigmahat)\n",
    "            except:\n",
    "                print(\"Some Other Error..\")\n",
    "            # store results\n",
    "            tmp_res[cur_modelname].append(np.std(fut_ret_mat @ weights, ddof=1) * np.sqrt(252) * 100)\n",
    "            if idx % 21 == 0:\n",
    "                tmp_rawres[cur_modelname] += list(fut_ret_mat @ weights)\n",
    "\n",
    "        if idx % 250 == 0:\n",
    "            print(f\"done {idx} out of {permnos.shape[0]}\")\n",
    "\n",
    "    return tmp_rawres, tmp_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d38eb38-9722-4e85-8971-4e4ab8f3b585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nintensities_dfs = {}\\nfor k, v in evs_dfs.items():\\n    v.index = pd.to_datetime(permnos.index, format=\"%Y%m%d\")\\n    intensities_dfs[k] = get_ev_intensities(v, evs_dfs[\"sample\"])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD EIGENVALUES\n",
    "PF_SIZE = 30\n",
    "\n",
    "model_names = [\"qis\",  \"sample\"]\n",
    "evs_dfs = {}\n",
    "qis_evs_path = r\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\NonLinear_Shrinkage\\transformed_qis_eigenvalues\"\n",
    "evs_dfs[\"qis\"] = pd.read_csv(qis_evs_path + f'\\\\qis_evs_df_p{PF_SIZE}.csv', index_col=0)\n",
    "evs_dfs[\"sample\"] = pd.read_csv(qis_evs_path + f'\\\\sample_evs_df_p{PF_SIZE}.csv', index_col=0)\n",
    "\n",
    "base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "permnos = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{PF_SIZE}.pickle\")\n",
    "'''\n",
    "intensities_dfs = {}\n",
    "for k, v in evs_dfs.items():\n",
    "    v.index = pd.to_datetime(permnos.index, format=\"%Y%m%d\")\n",
    "    intensities_dfs[k] = get_ev_intensities(v, evs_dfs[\"sample\"])\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f55b464f-d263-4717-90a4-ca078cfb5799",
   "metadata": {},
   "source": [
    "## Get eigenvalues via new formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "798f977b-0a47-44e4-8707-3e54e9564fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda_qis_nl(qis_ev, sample_ev, int_of_int):\n",
    "    a = ( (qis_ev/sample_ev - 1)*int_of_int + 1) * sample_ev\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_new_eigenvalues(qis_eigenvalues, sample_eigenvalues, intensity_of_intensity):\n",
    "    intens_df = qis_eigenvalues / sample_eigenvalues\n",
    "    if qis_eigenvalues.shape[1] == 500:  # only 1 year case ofcourse\n",
    "        intens_df = intens_df.iloc[:, -251:]\n",
    "\n",
    "\n",
    "     \n",
    "    right_rotation_idx = np.argmin( np.abs(intens_df-1) , axis=1)\n",
    "    left_rotation_idx = right_rotation_idx - 1\n",
    "    rotation_points = 0.5 * np.diag(intens_df.iloc[:, left_rotation_idx]) + 0.5 * np.diag(intens_df.iloc[:, right_rotation_idx])\n",
    "\n",
    "\n",
    "def get_new_eigenvalues(qis_eigenvalues, sample_eigenvalues, intensity_of_intensity):\n",
    "    intensity = qis_eigenvalues.copy()\n",
    "    if qis_eigenvalues.shape[1] == 500:\n",
    "        intensity.iloc[: , -251:] = qis_eigenvalues.iloc[: , -251:] / sample_eigenvalues.iloc[: , -251:]\n",
    "    else:\n",
    "        intensity = qis_eigenvalues / sample_eigenvalues\n",
    "    intensity_delta = intensity - 1\n",
    "    intensity_delta_new = intensity_delta * intensity_of_intensity\n",
    "    intensity_new = intensity_delta_new + 1\n",
    "    qis_evs_new = intensity_new * sample_eigenvalues\n",
    "    if qis_eigenvalues.shape[1] == 500:\n",
    "        qis_evs_new.iloc[: , 0:250] = intensity_of_intensity * qis_eigenvalues.iloc[: , 0:250]\n",
    "\n",
    "    # get Rotation Points\n",
    "    intens_df = qis_eigenvalues / sample_eigenvalues\n",
    "    if qis_eigenvalues.shape[1] == 500:  # only 1 year case ofcourse\n",
    "        intens_df = intens_df.fillna(0)\n",
    "    right_rotation_idx = np.argmin( np.abs(intens_df-1) , axis=1)\n",
    "    left_rotation_idx = right_rotation_idx - 1\n",
    "    rotation_point_sample_evs = 0.5 * np.diag(sample_eigenvalues.iloc[:, left_rotation_idx]) + 0.5 * np.diag(sample_eigenvalues.iloc[:, right_rotation_idx])\n",
    "    rotation_points = 0.5 * np.diag(intens_df.iloc[:, left_rotation_idx]) + 0.5 * np.diag(intens_df.iloc[:, right_rotation_idx])\n",
    "    rotation_points = rotation_points * rotation_point_sample_evs\n",
    "\n",
    "    # Force all eigenvalues to be in decreasing order (from the largest one)\n",
    "    idx_is_increasing = qis_evs_new.diff(axis=1).fillna(0) < 0\n",
    "    qis_evs_new[idx_is_increasing] = np.nan\n",
    "    qis_evs_new = qis_evs_new.bfill(axis=1)\n",
    "\n",
    "    # check if values left (right) of rotation point are smaller (larger) than rotation point\n",
    "    # check: is idx < left and value_at_idx > value_at_left --> then np.nan and ffill it for left (and bfill for right)\n",
    "    for i in range(qis_evs_new.shape[0]):\n",
    "        tmp = qis_evs_new.iloc[i, :].copy()\n",
    "        left_bool = tmp[0:left_rotation_idx[i]] > rotation_points[i]\n",
    "        right_bool = tmp[left_rotation_idx[i]:] < rotation_points[i]\n",
    "        # change those evs on right that are smaller than rotation point\n",
    "        tmp_right = tmp[left_rotation_idx[i]:]\n",
    "        tmp_right[right_bool] = np.nan\n",
    "        tmp_right.bfill(inplace=True) \n",
    "        tmp_right.ffill(inplace=True)  # in case the largest ev is NAN, we need to additionally ffill\n",
    "        # change those evs on left that are LARGER than rotation point\n",
    "        tmp_left = tmp[0:left_rotation_idx[i]]\n",
    "        tmp_left[left_bool] = np.nan\n",
    "        tmp_left.ffill(inplace=True) \n",
    "        tmp_left.bfill(inplace=True) # in case the smallest ev is NAN, we need to additionally bfill\n",
    "        tmp_left.fillna(rotation_points[i], inplace=True)  # in case all eigenvalues left of rotation point are larger than rotation point\n",
    "        # change qis_evs_new row\n",
    "        qis_evs_new.iloc[i, :] = np.concatenate([tmp_left, tmp_right])\n",
    "\n",
    "    # a few correction checks\n",
    "    assert any(qis_evs_new.diff(axis=1).fillna(0) < 0), \"Eigenvalues are not monotonically decreasing!\"\n",
    "    assert any(qis_evs_new.isna()), \"There are NaN's in the QIS Eigenvalue Matrix!\"\n",
    " \n",
    "    return qis_evs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e28d603-c8b2-4d35-a72d-4256d6a67231",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_of_intensity_list = np.arange(0.5, 2.01, 0.05).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008de4e6-de24-49b4-a488-4c4790a3ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "qis_evs_new = {}\n",
    "intensities_new = {}\n",
    "for intensity_of_intensity in intensity_of_intensity_list:\n",
    "    qis_evs_new[intensity_of_intensity] = get_new_eigenvalues(evs_dfs[\"qis\"], evs_dfs[\"sample\"], intensity_of_intensity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c136a4b-c603-4873-bd97-77f121412dfd",
   "metadata": {},
   "source": [
    "## Load return data and create training data for new qis eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f510a52-0135-45d6-ae0d-330fb5069a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "# IMPORT SHRK DATASETS\n",
    "pf_size = PF_SIZE  # DONT CHANGE HERE!!\n",
    "permnos = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{pf_size}.pickle\")\n",
    "rets_full = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\returns_full_1Y_p{pf_size}.pickle\")\n",
    "\n",
    "fixed_shrk_name = 'cov2Para'\n",
    "opt_shrk_name = 'cov2Para'\n",
    "with open(rf\"{base_folder_path}\\ONE_YR\\preprocessing\\training_dfs\\PF{pf_size}\\fixed_shrkges_cov2Para_p{pf_size}.pickle\", 'rb') as f:\n",
    "    fixed_shrk_data = pickle.load(f)\n",
    "with open(rf\"{base_folder_path}\\ONE_YR\\preprocessing\\training_dfs\\PF{pf_size}\\cov2Para_factor-1.0_p{pf_size}.pickle\", 'rb') as f:\n",
    "    optimal_shrk_data = pickle.load(f)\n",
    "\n",
    "# get all the validation indices\n",
    "len_train = 5040\n",
    "end_date = fixed_shrk_data.shape[0]\n",
    "# temp here\n",
    "val_indices_correct = (len_train, end_date)\n",
    "val_indices_results = [val_indices_correct[0] + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "val_idxes_shrkges = [0 + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "reb_date_1 = permnos.index[0]\n",
    "add_idx = np.where(rets_full.index == reb_date_1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5488a3d7-d201-4e09-9f27-3b1df753486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0 out of 10353\n",
      "done 250 out of 10353\n",
      "done 500 out of 10353\n",
      "done 750 out of 10353\n",
      "done 1000 out of 10353\n",
      "done 1250 out of 10353\n",
      "done 1500 out of 10353\n",
      "done 1750 out of 10353\n",
      "done 2000 out of 10353\n",
      "done 2250 out of 10353\n",
      "done 2500 out of 10353\n",
      "done 2750 out of 10353\n",
      "done 3000 out of 10353\n",
      "done 3250 out of 10353\n",
      "done 3500 out of 10353\n",
      "done 3750 out of 10353\n",
      "done 4000 out of 10353\n",
      "done 4250 out of 10353\n",
      "done 4500 out of 10353\n",
      "done 4750 out of 10353\n",
      "done 5000 out of 10353\n",
      "done 5250 out of 10353\n",
      "done 5500 out of 10353\n",
      "done 5750 out of 10353\n",
      "done 6000 out of 10353\n",
      "done 6250 out of 10353\n",
      "done 6500 out of 10353\n",
      "done 6750 out of 10353\n",
      "done 7000 out of 10353\n",
      "done 7250 out of 10353\n",
      "done 7500 out of 10353\n",
      "done 7750 out of 10353\n",
      "done 8000 out of 10353\n",
      "done 8250 out of 10353\n",
      "done 8500 out of 10353\n",
      "done 8750 out of 10353\n",
      "done 9000 out of 10353\n",
      "done 9250 out of 10353\n",
      "done 9500 out of 10353\n",
      "done 9750 out of 10353\n",
      "done 10000 out of 10353\n",
      "done 10250 out of 10353\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ioi_path = r\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\NonLinear_Shrinkage\\intensity_of_intensity_data\"\n",
    "    all_res = pd.read_csv(ioi_path + f\"\\\\all_res_p{PF_SIZE}_v2.csv\", index_col=0)\n",
    "    all_rawres = pd.read_csv(ioi_path + f\"\\\\all_rawres_p{PF_SIZE}_v2.csv\", index_col=0)\n",
    "except:\n",
    "    tmp_rawres, tmp_res =  get_rawres(qis_evs_new, modelnames = intensity_of_intensity_list)\n",
    "    all_res = pd.DataFrame(tmp_res.copy())\n",
    "    all_rawres = pd.DataFrame(tmp_rawres.copy())\n",
    "    out_path = r\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\NonLinear_Shrinkage\\intensity_of_intensity_data\"\n",
    "    all_res.to_csv(out_path + f\"\\\\all_res_p{PF_SIZE}_v2.csv\")\n",
    "    all_rawres.to_csv(out_path + f\"\\\\all_rawres_p{PF_SIZE}_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d658a1-5f93-47ef-ad74-3fb76ec665b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rawres.iloc[5040:, :].std()  * np.sqrt(252) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6afdf-ff60-43e0-be61-50b392763d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities={}\n",
    "for int in qis_evs_new.keys():\n",
    "    intensities[int] = qis_evs_new[int].iloc[:, -251:] / evs_dfs[\"sample\"].iloc[:, -251:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba63319-85cd-49c7-b682-2b48551ca6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "allres_min_idxes_full = all_res.idxmin(axis=1)[: -21].values\n",
    "allres_min_idxes_full = np.insert(allres_min_idxes_full, 0, np.repeat([\"1.0\"], 21))\n",
    "\n",
    "# for sanity check: BIASED version should generally be better than\n",
    "# non biased version as it is literally the minimum over the future 21 days\n",
    "# so using it as a signal should outperform\n",
    "allres_min_idxes_BIASED = all_res.idxmin(axis=1).values\n",
    "allres_min_idxes_BIASED = allres_min_idxes_BIASED\n",
    "\n",
    "\n",
    "# simple argmin rule, with full allres_min, should be same results as above\n",
    "allres_min_idxes_full_v2 = allres_min_idxes_full[list(range(0, allres_min_idxes_full.shape[0], 21))]\n",
    "allres_min_idxes_full_v2 = np.repeat(allres_min_idxes_full_v2, 21)\n",
    "res_simple_argmin_rule = np.diag(all_rawres.loc[:, allres_min_idxes_full_v2])[5040:]\n",
    "\n",
    "\n",
    "# simple argmin rule, biased (as a sanity check)\n",
    "allres_min_idxes_BIASED_v2 = allres_min_idxes_BIASED[list(range(0, allres_min_idxes_BIASED.shape[0], 21))]\n",
    "allres_min_idxes_BIASED_v2 = np.repeat(allres_min_idxes_BIASED_v2, 21)\n",
    "res_simple_argmin_rule_biased = np.diag(all_rawres.loc[:, allres_min_idxes_BIASED_v2])[5040:]\n",
    "\n",
    "res_actual_argmin = []\n",
    "for i in range(5313//21):\n",
    "    tmp_data = all_rawres.iloc[5040 + 21*i: 5040 + 21*(i+1)]\n",
    "    curmin_idx = tmp_data.std().idxmin()\n",
    "    curmin = tmp_data.loc[:, curmin_idx]\n",
    "    res_actual_argmin += curmin.tolist()\n",
    "# np.std(res_actual_argmin) * np.sqrt(252) * 100  --> 10.375\n",
    "\n",
    "res_actual_argmin_nonbiased = []\n",
    "for i in range(5313//21):\n",
    "    idx_min_data = all_rawres.iloc[5040 - 21 + 21*i: 5040 - 21 + 21*(i+1)]\n",
    "    curmin_idx = idx_min_data.std().idxmin()\n",
    "    tmp_data = all_rawres.iloc[5040 + 21*i: 5040 + 21*(i+1)]\n",
    "    curmin = tmp_data.loc[:, curmin_idx]\n",
    "    res_actual_argmin_nonbiased += curmin.tolist()\n",
    "#np.std(res_actual_argmin_nonbiased) * np.sqrt(252) * 100  --> 10.65\n",
    "\n",
    "#min_idxes = fixed_shrk_data.iloc[:, 3:].idxmin(axis=1)\n",
    "#opt_vals = np.diag(fixed_shrk_data.iloc[:, 3:].loc[:, min_idxes])\n",
    "\n",
    "# get all the validation indices\n",
    "len_train = 5040\n",
    "end_date = fixed_shrk_data.shape[0]\n",
    "# temp here\n",
    "val_indices_correct = (len_train, end_date)\n",
    "val_indices_results = [val_indices_correct[0] + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "val_idxes_shrkges = [0 + 21 * i for i in range((val_indices_correct[-1] - val_indices_correct[0]) // 21)]\n",
    "reb_date_1 = permnos.index[0]\n",
    "add_idx = np.where(rets_full.index == reb_date_1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0c689-248c-4110-b4c6-e2eac2993739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path\n",
    "os.chdir(r'H:\\all\\RL_Shrinkage_2024')\n",
    "from ONE_YR.NonLinear_Shrinkage import regression_evaluation_funcs as re_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec0c4f-c9c5-4067-be98-c4e5fbfe5fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = intensity_of_intensity_list\n",
    "Y = allres_min_idxes_BIASED.astype(float)\n",
    "opt_values = allres_min_idxes_BIASED.astype(float)[:-21]\n",
    "opt_values = np.insert(arr=opt_values, obj=0, values=np.repeat(0.0, 21))\n",
    "\n",
    "Y = np.array(re_hf.map_factors_to_preds(Y.reshape(-1), all_factors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfed666-7b8e-46b6-b06a-8dc6ebae48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_factors_to_preds(preds_as_floats, all_factors):\n",
    "    all_nums = np.arange(len(all_factors))\n",
    "    mapping = {}\n",
    "    for i, f in enumerate(all_factors):\n",
    "        mapping[f] = all_nums[i]\n",
    "    model_preds = list(map(lambda x: mapping[x], preds_as_floats))\n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302a108-27ee-41de-a4b8-a0ce3a65a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = intensity_of_intensity_list\n",
    "Y = allres_min_idxes_BIASED.astype(float)\n",
    "opt_values = allres_min_idxes_BIASED.astype(float)[:-21]\n",
    "opt_values = np.insert(arr=opt_values, obj=0, values=np.repeat(1.0, 21))\n",
    "Y = np.array(re_hf.map_factors_to_preds(Y.reshape(-1), all_factors))\n",
    "opt_values = np.array(re_hf.map_factors_to_preds(opt_values, all_factors))\n",
    "opt_v3 = np.diag(all_res.loc[:, allres_min_idxes_BIASED])[:-21]\n",
    "opt_v3 = np.insert(arr=opt_v3, obj=0, values=np.repeat(7.0, 21))\n",
    "\n",
    "#opt_v3 = pd.Series(opt_v3).rolling(window=40, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c4312-d776-4e1b-91c8-d8d262e0ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_v4 = np.copy(opt_v3)\n",
    "opt_v4 = np.insert(arr=opt_v3, obj=0, values=np.repeat(7.0, 21))[:-21]\n",
    "\n",
    "rolling_opt = pd.Series(opt_values).rolling(window=252, min_periods=1).mean().values\n",
    "rolling_opt2 = pd.Series(opt_v3).rolling(window=252, min_periods=1).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cde45-faeb-40b2-978d-de2233c8e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'pf_size' : pf_size,\n",
    "    'opt_values_factors' : opt_values,\n",
    "'include_ts_momentum_var_allstocks': False,\n",
    "'include_ts_momentum_allstocks': True,\n",
    "'include_sample_covmat_trace': True,\n",
    "'include_mean_of_correls': False,\n",
    "'include_iqr': False,\n",
    "'include_factors': False,\n",
    "'include_ewma_year': False,\n",
    "'include_ewma_month': True,\n",
    "'include_ew_year_vola': False,\n",
    "'include_ew_month_vola': True,\n",
    "'include_allstocks_year_avgvola': True,\n",
    "'include_allstocks_month_avgvola': False,\n",
    "    'additional_inputs' : [opt_v3]  # opt_v4, rolling_opt, rolling_opt2\n",
    "}\n",
    "\n",
    "X = re_hf.load_additional_train_data(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47ab4f-53f4-481d-9865-90d675fe15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_outputs(res, intensity_of_intensity_list, show_fig=True):\n",
    "    res = [len(intensity_of_intensity_list)-1 if r > len(intensity_of_intensity_list) else r for r in res]\n",
    "    res = [0 if r < 0 else r for r in res]\n",
    "    res = re_hf.map_preds_to_factors(res, all_factors)\n",
    "    Y_eval = all_rawres\n",
    "    Y_eval.columns = Y_eval.columns.astype(str)\n",
    "    res_evaluated2 = re_hf.evaluate_all_factor_preds(res, Y_eval, len_train)\n",
    "    datetime_index = pd.to_datetime(permnos.index, format=\"%Y%m%d\")\n",
    "    res_df = pd.DataFrame(np.array(res, dtype=float), index=datetime_index[5040:], columns=[\"\"])\n",
    "    \n",
    "    fig = px.line(res_df, height=500, width=1000).update_layout(\n",
    "        xaxis_title=\"Year\", yaxis_title=\"Intensity of Intensity\")\n",
    "    \n",
    "    print(\"Result:\", res_evaluated2)\n",
    "    if show_fig:\n",
    "        fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8031bd-db61-4d19-96a6-ea32cd9ba763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "res = re_hf.general_single_output_ElasticNet_Lagged(X=X, Y=Y, len_train=5040)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17c9b20-b120-40ce-a0f6-4e5547367273",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = eval_model_outputs(res, intensity_of_intensity_list, show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76412f41-cf70-4710-abb2-b88e9e89ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rawres.iloc[5040:, :].std() * np.sqrt(252) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390031f4-86c4-4948-b6a1-24c2c6a9a313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
