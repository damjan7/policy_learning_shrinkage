{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\dkostovic\\AppData\\Local\\Temp\\3\\ipykernel_55092\\2019214749.py\", line 17, in <module>\n",
      "    from helpers import helper_functions as hf\n",
      "  File \"H:\\all\\RL_Shrinkage_2024\\helpers\\helper_functions.py\", line 3, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "import psutil\n",
    "psutil.cpu_count()\n",
    "p = psutil.Process()\n",
    "p.cpu_affinity([0,1,2,3,4,5,6,7,8])\n",
    "\n",
    "os.chdir(r'H:\\all\\RL_Shrinkage_2024')\n",
    "from helpers import helper_functions as hf\n",
    "from ONE_YR.NonLinear_Shrinkage import regression_evaluation_funcs as re_hf\n",
    "from helpers import eval_function_new\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: We have the future return matrices available and we know what stocks are in the universe at each given time point. We need to extract the market capitalization for the stocks at each rebalancing in the out of sample period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load permnos, and extract the dates for which we need the market caps\n",
    "\n",
    "PF_SIZE = 30\n",
    "base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "# IMPORT SHRK DATASETS\n",
    "pf_size = PF_SIZE  # DONT CHANGE HERE!!\n",
    "permnos = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{pf_size}.pickle\")\n",
    "rets_full = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\returns_full_1Y_p{pf_size}.pickle\")\n",
    "market_caps = pd.read_pickle(\n",
    "    fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\oos_rebdate_marketcaps_1Y_p{pf_size}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "permnos.index = pd.to_datetime(permnos.index, format=\"%Y%m%d\")\n",
    "rebdates = permnos.index[list(range(5040, permnos.shape[0], 21))]\n",
    "assert len(rebdates) == 253, \"Rebdates is not of correct length..\"\n",
    "market_caps.index = pd.to_datetime(market_caps.index, format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rebdate in market_caps.index:\n",
    "    market_caps.loc[market_caps.index == rebdate, ~market_caps.columns.isin(permnos.loc[rebdate,:])] = pd.NA\n",
    "\n",
    "VW_weights = market_caps.T / market_caps.sum(axis=1)\n",
    "VW_weights = VW_weights.T\n",
    "\n",
    "EW_weights = market_caps.copy()\n",
    "EW_weights[~EW_weights.isna()] = 1/PF_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load permnos, and extract the dates for which we need the market caps\n",
    "base_folder_path = r'H:\\\\all\\\\RL_Shrinkage_2024'\n",
    "# IMPORT SHRK DATASETS\n",
    "\n",
    "VW_RES_DICT, EW_RES_DICT = {}, {}\n",
    "#for PF_SIZE in [30, 50, 100, 225, 500]:\n",
    "for PF_SIZE in [30]:\n",
    "    pf_size = PF_SIZE  # DONT CHANGE HERE!!\n",
    "    permnos = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{pf_size}.pickle\")\n",
    "    rets_full = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\returns_full_1Y_p{pf_size}.pickle\")\n",
    "    market_caps = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\oos_rebdate_marketcaps_1Y_p{pf_size}.pickle\")\n",
    "\n",
    "    permnos.index = pd.to_datetime(permnos.index, format=\"%Y%m%d\")\n",
    "    rebdates = permnos.index[list(range(5040, permnos.shape[0], 21))]\n",
    "    assert len(rebdates) == 253, \"Rebdates is not of correct length..\"\n",
    "    market_caps.index = pd.to_datetime(market_caps.index, format=\"%Y%m%d\")\n",
    "\n",
    "    for rebdate in market_caps.index:\n",
    "        market_caps.loc[market_caps.index == rebdate, ~market_caps.columns.isin(permnos.loc[rebdate,:])] = pd.NA\n",
    "\n",
    "    VW_weights = market_caps.T / market_caps.sum(axis=1)\n",
    "    VW_weights = VW_weights.T\n",
    "\n",
    "    EW_weights = market_caps.copy()\n",
    "    EW_weights[~EW_weights.isna()] = 1/PF_SIZE\n",
    "\n",
    "    # iterate through the return matrices\n",
    "    VW_rawres = []\n",
    "    EW_rawres = []\n",
    "    reb_date_1 = permnos.index[0]\n",
    "    add_idx = np.where(pd.to_datetime(rets_full.index, format=\"%Y%m%d\") == reb_date_1)[0][0]\n",
    "    for weights_idx, idx in enumerate(range(5040, permnos.shape[0], 21)):\n",
    "        #past_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx - 21 * 12 * 1: idx + add_idx, :]\n",
    "        #past_ret_mat = past_ret_mat.sub(past_ret_mat.mean())\n",
    "        #past_ret_mat = past_ret_mat.fillna(0)\n",
    "        fut_ret_mat = rets_full[permnos.iloc[idx]].iloc[idx + add_idx: idx + add_idx + 21, :]\n",
    "        VW_rawres += (fut_ret_mat @ VW_weights.iloc[weights_idx, :].loc[permnos.iloc[idx]].T).tolist()\n",
    "        EW_rawres += (fut_ret_mat @ EW_weights.iloc[weights_idx, :].loc[permnos.iloc[idx]].T).tolist()\n",
    "    \n",
    "    VW_RES_DICT[PF_SIZE] = VW_rawres\n",
    "    EW_RES_DICT[PF_SIZE] = EW_rawres\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5118799732335056)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.mean(EW_RES_DICT[30]) * (252) * 100) / (np.std(VW_RES_DICT[30]) * np.sqrt(252) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.79134071146245)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(EW_RES_DICT[30]) * (252) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "VW_RES_DF = pd.DataFrame(VW_RES_DICT)\n",
    "VW_RES_DF.index = permnos.index[5040:]\n",
    "\n",
    "EW_RES_DF = pd.DataFrame(EW_RES_DICT)\n",
    "EW_RES_DF.index = permnos.index[5040:]\n",
    "\n",
    "VW_RES_DF.to_csv(r\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\preprocessing\\rets_permnos_1Y\\VW_portfolio_daily_returns.csv\")\n",
    "EW_RES_DF.to_csv(r\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\preprocessing\\rets_permnos_1Y\\EW_portfolio_daily_returns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AV</th>\n",
       "      <th>SD</th>\n",
       "      <th>IR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9.79</td>\n",
       "      <td>18.72</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>9.88</td>\n",
       "      <td>18.86</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>9.97</td>\n",
       "      <td>19.51</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>10.90</td>\n",
       "      <td>19.87</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>12.01</td>\n",
       "      <td>20.62</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AV     SD    IR\n",
       "30    9.79  18.72  0.52\n",
       "50    9.88  18.86  0.52\n",
       "100   9.97  19.51  0.51\n",
       "225  10.90  19.87  0.55\n",
       "500  12.01  20.62  0.58"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create matrix containing SD's and AV's for convex combinations\n",
    "\n",
    "RES = {}\n",
    "RES['AV'], RES['SD'] = {}, {}\n",
    "for PF_SIZE in [30, 50, 100, 225, 500]:\n",
    "    RES['SD'][PF_SIZE] = EW_RES_DF[PF_SIZE].std() * np.sqrt(252) * 100\n",
    "    RES['AV'][PF_SIZE] = EW_RES_DF[PF_SIZE].mean() * (252) * 100\n",
    "\n",
    "RES_DF = pd.DataFrame(RES)\n",
    "RES_DF['IR'] = RES_DF['AV'] / RES_DF['SD']\n",
    "\n",
    "RES_DF.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07, 0.07, 0.07, ..., 0.06, 0.06, 0.06])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ints.round(2)['shrk_factor'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(11.013846795392254)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf_size=500\n",
    "with open(rf\"{base_folder_path}\\ONE_YR\\preprocessing\\training_dfs\\PF{pf_size}\\fixed_shrkges_rawres_cov1Para_p{pf_size}.pickle\", 'rb') as f:\n",
    "    rawres_fixed_shrk_data = pickle.load(f)\n",
    "\n",
    "ints = pd.read_csv(rf\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\Linear_Shrinkage\\results\\p500\\c1p_p500_intensities.csv\", index_col=0)\n",
    "\n",
    "\n",
    "ints.round(2)\n",
    "\n",
    "np.diag(rawres_fixed_shrk_data.loc[:, ints.round(2)['shrk_factor'].values])[5040:].std() * np.sqrt(252)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1p = []\n",
    "for pf_size in [30,50,100,225,500]:\n",
    "    x = pd.read_csv(rf\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\Linear_Shrinkage\\results\\p{pf_size}\\c1p_p{pf_size}_daily_returns.csv\")\n",
    "    c1p.append(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(5, 10353, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[216], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pd\u001b[39m.\u001b[39;49mDataFrame(c1p, columns\u001b[39m=\u001b[39;49m[\u001b[39m30\u001b[39;49m,\u001b[39m50\u001b[39;49m,\u001b[39m100\u001b[39;49m,\u001b[39m225\u001b[39;49m,\u001b[39m500\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\pandas\\core\\frame.py:867\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    859\u001b[0m         mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    860\u001b[0m             arrays,\n\u001b[0;32m    861\u001b[0m             columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 867\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    868\u001b[0m             data,\n\u001b[0;32m    869\u001b[0m             index,\n\u001b[0;32m    870\u001b[0m             columns,\n\u001b[0;32m    871\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    872\u001b[0m             copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    873\u001b[0m             typ\u001b[39m=\u001b[39;49mmanager,\n\u001b[0;32m    874\u001b[0m         )\n\u001b[0;32m    875\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    876\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    877\u001b[0m         {},\n\u001b[0;32m    878\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    882\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\pandas\\core\\internals\\construction.py:319\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    314\u001b[0m     values \u001b[39m=\u001b[39m _ensure_2d(values)\n\u001b[0;32m    316\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39m# by definition an array here\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[39m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m     values \u001b[39m=\u001b[39m _prep_ndarraylike(values, copy\u001b[39m=\u001b[39;49mcopy_on_sanitize)\n\u001b[0;32m    321\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m values\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m dtype:\n\u001b[0;32m    322\u001b[0m     \u001b[39m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     values \u001b[39m=\u001b[39m sanitize_array(\n\u001b[0;32m    324\u001b[0m         values,\n\u001b[0;32m    325\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    328\u001b[0m         allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\pandas\\core\\internals\\construction.py:582\u001b[0m, in \u001b[0;36m_prep_ndarraylike\u001b[1;34m(values, copy)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     values \u001b[39m=\u001b[39m convert(values)\n\u001b[1;32m--> 582\u001b[0m \u001b[39mreturn\u001b[39;00m _ensure_2d(values)\n",
      "File \u001b[1;32mc:\\Users\\dkostovic\\.conda\\envs\\FTSE_data_analysis_v2\\lib\\site-packages\\pandas\\core\\internals\\construction.py:592\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    590\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mreshape((values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m))\n\u001b[0;32m    591\u001b[0m \u001b[39melif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMust pass 2-d input. shape=\u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    593\u001b[0m \u001b[39mreturn\u001b[39;00m values\n",
      "\u001b[1;31mValueError\u001b[0m: Must pass 2-d input. shape=(5, 10353, 6)"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(c1p, columns=[30,50,100,225,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AV</th>\n",
       "      <th>SD</th>\n",
       "      <th>IR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29.061739</td>\n",
       "      <td>6.651737</td>\n",
       "      <td>4.369045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>27.840844</td>\n",
       "      <td>7.035900</td>\n",
       "      <td>3.956970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>19.018341</td>\n",
       "      <td>6.825734</td>\n",
       "      <td>2.786270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>23.284861</td>\n",
       "      <td>6.417609</td>\n",
       "      <td>3.628277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>26.543277</td>\n",
       "      <td>6.240346</td>\n",
       "      <td>4.253494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AV        SD        IR\n",
       "30   29.061739  6.651737  4.369045\n",
       "50   27.840844  7.035900  3.956970\n",
       "100  19.018341  6.825734  2.786270\n",
       "225  23.284861  6.417609  3.628277\n",
       "500  26.543277  6.240346  4.253494"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1p = pd.read_csv(r\"H:\\all\\RL_Shrinkage_2024\\ONE_YR\\Linear_Shrinkage\\results\\cov1para_daily_returns.csv\", index_col=0)\n",
    "\n",
    "c1p_res = {}\n",
    "\n",
    "c1p_res[\"AV\"] = c1p.iloc[5040:, :].mean() * (252)\n",
    "c1p_res[\"SD\"] = c1p.iloc[5040:, :].std() * np.sqrt(252) \n",
    "c1p_res[\"IR\"] = c1p_res[\"AV\"]  / c1p_res[\"SD\"]\n",
    "\n",
    "pd.DataFrame(c1p_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AV</th>\n",
       "      <th>SD</th>\n",
       "      <th>IR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-34.14</td>\n",
       "      <td>317.04</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10.05</td>\n",
       "      <td>19.04</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>10.04</td>\n",
       "      <td>19.20</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>10.35</td>\n",
       "      <td>19.32</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>10.71</td>\n",
       "      <td>19.52</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AV      SD    IR\n",
       "30  -34.14  317.04 -0.11\n",
       "50   10.05   19.04  0.53\n",
       "100  10.04   19.20  0.52\n",
       "225  10.35   19.32  0.54\n",
       "500  10.71   19.52  0.55"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create matrix containing SD's and AV's for convex combinations\n",
    "\n",
    "RES = {}\n",
    "RES['AV'], RES['SD'] = {}, {}\n",
    "for PF_SIZE in [30, 50, 100, 225, 500]:\n",
    "    RES['SD'][PF_SIZE] = VW_RES_DF[PF_SIZE].std() * np.sqrt(252) * 100\n",
    "    RES['AV'][PF_SIZE] = VW_RES_DF[PF_SIZE].mean() * (252) * 100\n",
    "\n",
    "RES_DF = pd.DataFrame(RES)\n",
    "RES_DF['IR'] = RES_DF['AV'] / RES_DF['SD']\n",
    "\n",
    "RES_DF.round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the EW and VW out of sample results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Portfolio Turnover Metrics for EW and VW portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_portfolio_stats(df):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "      1) TO = average (monthly) turnover\n",
    "      2) GL = average (monthly) gross leverage\n",
    "      3) PL = average (monthly) proportion of leverage (fraction of short positions)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Fill NaNs with zero\n",
    "    df_filled = df.fillna(0)\n",
    "    \n",
    "    # 2. Compute average turnover (TO)\n",
    "    #\n",
    "    #    Turnover at date h is ||w_{h+1} - w_h||_1, i.e. sum of absolute differences\n",
    "    #    across all stocks. Then we average over h = 1..(T-1).\n",
    "    #\n",
    "    #    diff() yields row_{h+1} - row_h for each column; then we take abs() and sum across columns.\n",
    "    #    We skip the first NaN in diff() by taking [1:] and finally compute the mean.\n",
    "    \n",
    "    turnover_series = df_filled.diff().abs().sum(axis=1)      # || w_{h+1} - w_h ||_1 for each row\n",
    "    TO = turnover_series[1:].mean()                           # average over h=1..T-1\n",
    "    \n",
    "    # 3. Compute average gross leverage (GL)\n",
    "    #\n",
    "    #    Gross leverage at date h is ||w_h||_1, i.e. the sum of absolute values of the weights on date h.\n",
    "    #    We then average over all dates h.\n",
    "    \n",
    "    gross_leverage_series = df_filled.abs().sum(axis=1)       # || w_h ||_1 for each row\n",
    "    GL = gross_leverage_series.mean()                         # average over all rows\n",
    "    \n",
    "    # 4. Compute average proportion of leverage (PL)\n",
    "    #\n",
    "    #    Proportion of leverage is the fraction of negative weights (short positions). \n",
    "    #    That is 1/(T * p) * sum_{h=1}^T sum_{i=1}^p 1{ w_{i,h} < 0 }, where p=#stocks, T=#dates.\n",
    "    \n",
    "    # total number of negative entries:\n",
    "    num_negative = (df_filled < 0).sum().sum()\n",
    "    # total number of \"positions\" = T * p:\n",
    "    total_positions = df_filled.shape[0] * df_filled.shape[1]\n",
    "    PL = num_negative / total_positions\n",
    "    \n",
    "    return TO, GL, PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "VW_results, EW_results = {}, {}\n",
    "for PF_SIZE in [30, 50, 100, 225, 500]:\n",
    "    pf_size = PF_SIZE  # DONT CHANGE HERE!!\n",
    "    permnos = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\permnos_1Y_p{pf_size}.pickle\")\n",
    "    rets_full = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\returns_full_1Y_p{pf_size}.pickle\")\n",
    "    market_caps = pd.read_pickle(\n",
    "        fr\"{base_folder_path}\\ONE_YR\\preprocessing\\rets_permnos_1Y\\oos_rebdate_marketcaps_1Y_p{pf_size}.pickle\")\n",
    "\n",
    "    permnos.index = pd.to_datetime(permnos.index, format=\"%Y%m%d\")\n",
    "    rebdates = permnos.index[list(range(5040, permnos.shape[0], 21))]\n",
    "    assert len(rebdates) == 253, \"Rebdates is not of correct length..\"\n",
    "    market_caps.index = pd.to_datetime(market_caps.index, format=\"%Y%m%d\")\n",
    "\n",
    "    for rebdate in market_caps.index:\n",
    "        market_caps.loc[market_caps.index == rebdate, ~market_caps.columns.isin(permnos.loc[rebdate,:])] = pd.NA\n",
    "\n",
    "    VW_weights = market_caps.T / market_caps.sum(axis=1)\n",
    "    VW_weights = VW_weights.T\n",
    "\n",
    "    EW_weights = market_caps.copy()\n",
    "    EW_weights[~EW_weights.isna()] = 1/PF_SIZE\n",
    "\n",
    "    TO, GL, PL = compute_portfolio_stats(VW_weights)\n",
    "    VW_results[PF_SIZE] = [TO, GL, PL]\n",
    "\n",
    "    TO, GL, PL = compute_portfolio_stats(EW_weights)\n",
    "    EW_results[PF_SIZE] = [TO, GL, PL]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EW and VW Turnover Numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TO</th>\n",
       "      <th>GL</th>\n",
       "      <th>PL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TO   GL   PL\n",
       "30   0.06  1.0  0.0\n",
       "50   0.06  1.0  0.0\n",
       "100  0.06  1.0  0.0\n",
       "225  0.05  1.0  0.0\n",
       "500  0.05  1.0  0.0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EW_results_df = pd.DataFrame(EW_results, index=[\"TO\", \"GL\", \"PL\"])\n",
    "EW_results_df.T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TO</th>\n",
       "      <th>GL</th>\n",
       "      <th>PL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TO   GL   PL\n",
       "30   0.07  1.0  0.0\n",
       "50   0.07  1.0  0.0\n",
       "100  0.07  1.0  0.0\n",
       "225  0.06  1.0  0.0\n",
       "500  0.06  1.0  0.0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VW_results_df = pd.DataFrame(VW_results, index=[\"TO\", \"GL\", \"PL\"])\n",
    "VW_results_df.T.round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FTSE_data_analysis_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
